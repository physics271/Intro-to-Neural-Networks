{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Learning\n",
    "\n",
    "## Data\n",
    "\n",
    "[OPUS](http://opus.lingfil.uu.se/) (Open Parallel Corpus) provides many free parallel corpora. In particular, we'll use their [English-German Tatoeba corpus](http://opus.lingfil.uu.se/) which consists of phrases translated from English to German or vice-versa.\n",
    "\n",
    "Some preprocessing was involved to extract just the aligned sentences from the various XML files OPUS provides; I've provided the [processed data for you](../data/en_de_corpus.json).\n",
    "\n",
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Activation, Dense, RepeatVector, Input, Concatenate, SimpleRNN, Dropout, Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open('en_de_corpus.json', 'r'))\n",
    "\n",
    "# to deal with memory issues,\n",
    "# limit the dataset\n",
    "# we could also generate the training samples on-demand\n",
    "# with a generator and use keras models' `fit_generator` method\n",
    "max_len = 6\n",
    "max_examples = 80000\n",
    "max_vocab_size = 10000\n",
    "\n",
    "def get_texts(source_texts, target_texts, max_len, max_examples):\n",
    "    \"\"\"extract texts\n",
    "    training gets difficult with widely varying lengths\n",
    "    since some sequences are mostly padding\n",
    "    long sequences get difficult too, so we are going\n",
    "    to cheat and just consider short-ish sequences.\n",
    "    this assumes whitespace as a token delimiter\n",
    "    and that the texts are already aligned.\n",
    "    \"\"\"\n",
    "    sources, targets = [], []\n",
    "    for i, source in enumerate(source_texts):\n",
    "        # assume we split on whitespace\n",
    "        if len(source.split(' ')) <= max_len:\n",
    "            target = target_texts[i]\n",
    "            if len(target.split(' ')) <= max_len:\n",
    "                sources.append(source)\n",
    "                targets.append(target)\n",
    "    return sources[:max_examples], targets[:max_examples]\n",
    "\n",
    "en_texts, de_texts = get_texts(data['en'], data['de'], max_len, max_examples)\n",
    "n_examples = len(en_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and stop tokens\n",
    "start_token = '^'\n",
    "end_token = '$'\n",
    "en_texts = [' '.join([start_token, text, end_token]) for text in en_texts]\n",
    "de_texts = [' '.join([start_token, text, end_token]) for text in de_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters for the tokenizers to filter out\n",
    "# preserve start and stop tokens\n",
    "filter_chars = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n\\'`“”–'.replace(start_token, '').replace(end_token, '')\n",
    "\n",
    "source_tokenizer = Tokenizer(max_vocab_size, filters=filter_chars)\n",
    "source_tokenizer.fit_on_texts(en_texts)\n",
    "target_tokenizer = Tokenizer(max_vocab_size, filters=filter_chars)\n",
    "target_tokenizer.fit_on_texts(de_texts)\n",
    "\n",
    "# vocab sizes\n",
    "# idx 0 is reserved by keras (for padding)\n",
    "# and not part of the word_index,\n",
    "# so add 1 to account for it\n",
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max length (in tokens) of input and output sentences\n",
    "max_input_length = max(len(seq) for seq in source_tokenizer.texts_to_sequences_generator(en_texts))\n",
    "max_output_length = max(len(seq) for seq in target_tokenizer.texts_to_sequences_generator(de_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^ I took the bus back. $\n",
      "[  0   0   0   0   1   3 217   5 448 112   2]\n"
     ]
    }
   ],
   "source": [
    "sequences = pad_sequences(source_tokenizer.texts_to_sequences(en_texts[:1]), maxlen=max_input_length)\n",
    "print(en_texts[0])\n",
    "# >>> ^ I took the bus back. $\n",
    "print(sequences[0])\n",
    "# >>> [  0   0   0   2   4 223   3 461 114   1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_one_hot_vecs(sequences):\n",
    "    \"\"\"generate one-hot vectors from token sequences\"\"\"\n",
    "    # boolean to reduce memory footprint\n",
    "    X = np.zeros((len(sequences), max_input_length, source_vocab_size), dtype=np.bool)\n",
    "    for i, sent in enumerate(sequences):\n",
    "        word_idxs = np.arange(max_input_length)\n",
    "        X[i][[word_idxs, sent]] = True\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target_vecs():\n",
    "    \"\"\"encode words in the target sequences as one-hots\"\"\"\n",
    "    y = np.zeros((n_examples, max_output_length, target_vocab_size), dtype=np.bool)\n",
    "    for i, sent in enumerate(pad_sequences(target_tokenizer.texts_to_sequences(de_texts), maxlen=max_output_length)):\n",
    "        word_idxs = np.arange(max_output_length)\n",
    "        y[i][[word_idxs, sent]] = True\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "embedding_dim = 128\n",
    "\n",
    "def build_model(one_hot=False, bidirectional=False, extra_dense=False, random_stuff=False):\n",
    "    \"\"\"build a vanilla sequence-to-sequence model.\n",
    "    specify `one_hot=True` to build it for one-hot encoded inputs,\n",
    "    otherwise, pass in sequences directly and embeddings will be learned.\n",
    "    specify `bidirectional=False` to use a bidirectional LSTM\"\"\"\n",
    "    if one_hot:\n",
    "        input = Input(shape=(max_input_length,source_vocab_size))\n",
    "        input_ = input\n",
    "    else:\n",
    "        input = Input(shape=(max_input_length,), dtype='int32')\n",
    "        input_ = Embedding(source_vocab_size, embedding_dim, input_length=max_input_length)(input)\n",
    "\n",
    "    # encoder; don't return sequences, just give us one representation vector\n",
    "    if bidirectional:\n",
    "        forwards = LSTM(hidden_dim, return_sequences=False)(input_)\n",
    "        backwards = LSTM(hidden_dim, return_sequences=False, go_backwards=True)(input_)\n",
    "        encoder = Concatenate(-1)([forwards, backwards])\n",
    "    else:\n",
    "        encoder = LSTM(hidden_dim, return_sequences=False)(input_)\n",
    "        \n",
    "    # random extra dense layer\n",
    "    if extra_dense:\n",
    "        encoder = Dense(hidden_dim)(encoder)\n",
    "\n",
    "    # repeat encoder output for each desired output from the decoder\n",
    "    encoder = RepeatVector(max_output_length)(encoder)\n",
    "    \n",
    "    # decoder; do return sequences (timesteps)\n",
    "    decoder = LSTM(hidden_dim, return_sequences=True)(encoder)\n",
    "\n",
    "    # apply the dense layer to each timestep\n",
    "    # give output conforming to target vocab size\n",
    "    decoder = TimeDistributed(Dense(target_vocab_size))(decoder)\n",
    "    \n",
    "    # just some random layers to test out adding stuff\n",
    "    if random_stuff:\n",
    "        random_layer_1 = Dense(embedding_dim, activation='sigmoid')(decoder)\n",
    "        random_layer_2 = SimpleRNN(embedding_dim, activation='relu')(random_layer_1)\n",
    "        random_layer_3 = Average()([random_layer_1, random_layer_2])\n",
    "        decoder = Dropout(0.3)(random_layer_3)\n",
    "\n",
    "    # convert to a proper distribution\n",
    "    predictions = Activation('softmax')(decoder)\n",
    "    return Model(input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reverse_word_index = {v:k for k,v in target_tokenizer.word_index.items()}\n",
    "\n",
    "def decode_outputs(predictions):\n",
    "    outputs = []\n",
    "    for probs in predictions:\n",
    "        preds = probs.argmax(axis=-1)\n",
    "        tokens = []\n",
    "        for idx in preds:\n",
    "            tokens.append(target_reverse_word_index.get(idx))\n",
    "        outputs.append(' '.join([t for t in tokens if t is not None]))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq_vecs(sequences):\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def generate_batches(batch_size, one_hot=False):\n",
    "    # each epoch\n",
    "    n_batches = math.ceil(n_examples/batch_size)\n",
    "    while True:\n",
    "        sequences = pad_sequences(source_tokenizer.texts_to_sequences(en_texts), maxlen=max_input_length)\n",
    "\n",
    "        if one_hot:\n",
    "            X = build_one_hot_vecs(sequences)\n",
    "        else:\n",
    "            X = build_seq_vecs(sequences)\n",
    "        y = build_target_vecs()\n",
    "\n",
    "        # shuffle\n",
    "        idx = np.random.permutation(len(sequences))\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            start = batch_size * i\n",
    "            end = start+batch_size\n",
    "            yield X[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building some Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        [(None, 11, 11464)]       0         \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 128)               5935616   \n",
      "_________________________________________________________________\n",
      "repeat_vector_29 (RepeatVect (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_76 (LSTM)               (None, 11, 128)           131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 11, 19102)         2464158   \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 11, 19102)         0         \n",
      "=================================================================\n",
      "Total params: 8,531,358\n",
      "Trainable params: 8,531,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "model = build_model(one_hot=True, bidirectional=False, extra_dense=False)\n",
    "\n",
    "model.summary()\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.fit_generator(generate_batches(batch_size, one_hot=True), n_examples, n_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        [(None, 11)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 11, 128)           1467392   \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "repeat_vector_30 (RepeatVect (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 11, 128)           131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 11, 19102)         2464158   \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 11, 19102)         0         \n",
      "=================================================================\n",
      "Total params: 4,211,230\n",
      "Trainable params: 4,211,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(one_hot=False, bidirectional=False, extra_dense=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_51\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_38 (InputLayer)           [(None, 11, 11464)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_79 (LSTM)                  (None, 128)          5935616     input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_80 (LSTM)                  (None, 128)          5935616     input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           lstm_79[0][0]                    \n",
      "                                                                 lstm_80[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 128)          32896       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_31 (RepeatVector) (None, 11, 128)      0           dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_81 (LSTM)                  (None, 11, 128)      131584      repeat_vector_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, 11, 19102)    2464158     lstm_81[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 11, 19102)    0           time_distributed_31[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 14,499,870\n",
      "Trainable params: 14,499,870\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(one_hot=True, bidirectional=True, extra_dense=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_53\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 11, 128)      1467392     input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_82 (LSTM)                  (None, 128)          131584      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_83 (LSTM)                  (None, 128)          131584      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           lstm_82[0][0]                    \n",
      "                                                                 lstm_83[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_32 (RepeatVector) (None, 11, 256)      0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_84 (LSTM)                  (None, 11, 128)      197120      repeat_vector_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, 11, 19102)    2464158     lstm_84[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 11, 128)      2445184     time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)        (None, 128)          32896       dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_3 (Average)             (None, 11, 128)      0           dense_57[0][0]                   \n",
      "                                                                 simple_rnn_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 11, 128)      0           average_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 11, 128)      0           dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,869,918\n",
      "Trainable params: 6,869,918\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(one_hot=False, bidirectional=True, random_stuff=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "Note that, since training a model takes far too long for me on this computer, none of the subsequent cells have actually been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentences, one_hot=False):\n",
    "    seqs = pad_sequences(source_tokenizer.texts_to_sequences(sentences), maxlen=max_input_length)\n",
    "    if one_hot:\n",
    "        input = build_one_hot_vecs(seqs)\n",
    "    else:\n",
    "        input = build_seq_vecs(seqs)\n",
    "    preds = model.predict(input, verbose=0)\n",
    "    return decode_outputs(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(en_texts[0])\n",
    "print(de_texts[0])\n",
    "print(translate(model, [en_texts[0]], one_hot=True))\n",
    "# >>> ^ I took the bus back. $\n",
    "# >>> ^ Ich nahm den Bus zurück. $\n",
    "# >>> ^ ich ich die die verloren $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(one_hot=False, bidirectional=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(generator=generate_batches(batch_size, one_hot=False), samples_per_epoch=n_examples, nb_epoch=n_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(one_hot=False, bidirectional=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(generator=generate_batches(batch_size, one_hot=False), samples_per_epoch=n_examples, nb_epoch=n_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information\n",
    "\n",
    "- The underlying data is text. Specificially, the training data is pairs of translated English/German sentences.\n",
    "- First the underlying text data is filtered for size (this is for memory/efficiency's sake). Then, tokens are added to mark the start and end of samples (this also involves removing the tokens if they show up in the samples). Finally, the samples of words are transformed into vectors (potentially one-hot depending on the setup).\n",
    "- The output of the model is another vector which can be trnasformed back into a (German) sentence by basically reversing the process used to generate the input vectors (though obviously there are some differences since the language has changed).\n",
    "- The basic, one-hot version of the model has 4 hidden layers. These are an LSTM encoding layer, a RepeatVector (copying) layer, an LSTM decoding layer, and a TimeDistributed dense layer.\n",
    "- The final activation function used is a softmax function.\n",
    "- The loss function is something called Categorical Cross-entropy.\n",
    "- The only validation metric included was a graph of the accuracy across the epochs of various configurations for the model. Unfortunately, this graph just seems to be an image included in the notebook, so no code on how it was generated was provided. Apparently, after 300 epochs, an accuracy of ~82% was achieved.\n",
    "- Beyond just a letter by letter measure of accuracy, something like a whole-word measure of accuracy would probably be more helpful, as it punishes misspellings much more harshly. Also, one (admittedly much more complicated and somewhat impractical) idea would be to train a German to English translation network, and see if it is able to recover the original English phrases using this network's translations.\n",
    "- The idea behind this particular architecture is probably that the encoding step takes in the English words and reduces them down (in some fashion) to their actual meaning, while the decoding layer can then take that fundamental meaning and turn it into German words. The other layers just fascilitate this.\n",
    "- One thing that might be interesting to see is if one used the input layer as a further input to the decoding layer, as it somehow might be possible that knowing both the 'meaning of the sentence' (from the encoding layer) and the sentence itself could improve the translations. This could probably be achieved with some type of concatenation layer (I'm not quite sure what the proper architecture would be here)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
